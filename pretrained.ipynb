{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 1) config\n",
    "# -----------------\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# dataset path - change if needed\n",
    "data_dir = r\"C:/Users/soham/Desktop/food_class_mini_proj/food-11\"\n",
    "train_folder = os.path.join(data_dir, \"training\")\n",
    "test_folder = os.path.join(data_dir, \"evaluation\")\n",
    "\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "img_size = 224\n",
    "EPOCHS_HEAD = 6\n",
    "EPOCHS_FINETUNE = 6\n",
    "save_path = \"./food_resnet18.pth\"\n",
    "best_save_path = \"./food_resnet18_best.pth\"\n",
    "FINETUNE = True\n",
    "UNFREEZE_LAST_N_LAYERS = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 2) transforms + dataloaders\n",
    "# -----------------\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(0.1,0.1,0.1,0.05),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "])\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_folder, transform=train_transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "testset = torchvision.datasets.ImageFolder(root=test_folder, transform=val_transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "classes = trainset.classes\n",
    "print(\"detected classes (order):\", classes)\n",
    "print(\"train samples:\", len(trainset), \"test samples:\", len(testset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 3) visualize a few samples\n",
    "# -----------------\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "plt.figure(figsize=(12,4))\n",
    "for i in range(min(8, len(images))):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    img = images[i].cpu()\n",
    "    # unnormalize for display (imagenet mean/std)\n",
    "    img = img * torch.tensor([0.229,0.224,0.225]).view(3,1,1) + torch.tensor([0.485,0.456,0.406]).view(3,1,1)\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.title(classes[labels[i]])\n",
    "    plt.axis('off')\n",
    "plt.suptitle('sample training images', size=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 4) build resnet18 (transfer learning)\n",
    "# -----------------\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "model = resnet18(weights=weights)\n",
    "# freeze backbone\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "# replace final layer\n",
    "model.fc = nn.Linear(model.fc.in_features, len(classes))\n",
    "model = model.to(device)\n",
    "print(model.fc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# helpers: train / eval\n",
    "# -----------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "def eval_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    if total==0:\n",
    "        return 0,0, np.array([]), np.array([])\n",
    "    return running_loss/total, correct/total, np.concatenate(all_preds), np.concatenate(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 5) train head (fc) only\n",
    "# -----------------\n",
    "best_acc = 0.0\n",
    "history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "for epoch in range(EPOCHS_HEAD):\n",
    "    train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, _, _ = eval_model(model, testloader, criterion, device)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    print(f\"head epoch {epoch+1}/{EPOCHS_HEAD} - train_acc={train_acc:.3f} val_acc={val_acc:.3f} train_loss={train_loss:.4f} val_loss={val_loss:.4f}\")\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), best_save_path)\n",
    "        print(\"saved best head model ->\", best_save_path)\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(\"saved head model to\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 6) optional fine-tune last layers + fc\n",
    "# -----------------\n",
    "if FINETUNE:\n",
    "    if os.path.exists(best_save_path):\n",
    "        model.load_state_dict(torch.load(best_save_path, map_location=device))\n",
    "        print(\"loaded best head model for finetuning\")\n",
    "    # unfreeze last n resnet blocks\n",
    "    layers = [model.layer1, model.layer2, model.layer3, model.layer4]\n",
    "    n = max(1, min(UNFREEZE_LAST_N_LAYERS, len(layers)))\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for layer in layers[-n:]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = True\n",
    "    for p in model.fc.parameters():\n",
    "        p.requires_grad = True\n",
    "    ft_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(ft_params, lr=1e-4)\n",
    "    print(f\"finetuning last {n} resnet blocks + fc, trainable params: {sum(p.numel() for p in ft_params)}\")\n",
    "    best_acc_ft = best_acc\n",
    "    for epoch in range(EPOCHS_FINETUNE):\n",
    "        train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer, device)\n",
    "        val_loss, val_acc, _, _ = eval_model(model, testloader, criterion, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        print(f\"finetune epoch {epoch+1}/{EPOCHS_FINETUNE} - train_acc={train_acc:.3f} val_acc={val_acc:.3f}\")\n",
    "        if val_acc > best_acc_ft:\n",
    "            best_acc_ft = val_acc\n",
    "            torch.save(model.state_dict(), best_save_path)\n",
    "            print(\"saved best finetuned model ->\", best_save_path)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(\"saved finetuned model to\", save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 7) evaluation & metrics (full test set)\n",
    "# -----------------\n",
    "if os.path.exists(best_save_path):\n",
    "    model.load_state_dict(torch.load(best_save_path, map_location=device))\n",
    "    print(\"loaded best model for final evaluation ->\", best_save_path)\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(testloader, desc='evaluating'):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "preds = np.concatenate(all_preds) if len(all_preds)>0 else np.array([])\n",
    "labels = np.concatenate(all_labels) if len(all_labels)>0 else np.array([])\n",
    "\n",
    "if preds.size == 0:\n",
    "    print('no predictions (empty testset?)')\n",
    "else:\n",
    "    overall_acc = accuracy_score(labels, preds)\n",
    "    report = classification_report(labels, preds, target_names=classes, digits=4)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    per_class_acc = cm.diagonal() / (cm.sum(axis=1) + 1e-12)\n",
    "    print(f\"overall accuracy: {overall_acc:.4f}\")\n",
    "    print('\\nclassification report:\\n', report)\n",
    "    for i, c in enumerate(classes):\n",
    "        print(f\"{c:20s} -> accuracy: {per_class_acc[i]:.4f} (n={int(cm.sum(axis=1)[i])})\")\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    with open('results/metrics_resnet.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"overall_accuracy: {overall_acc:.6f}\\n\\n\")\n",
    "        f.write('classification_report:\\n')\n",
    "        f.write(report)\n",
    "        f.write('\\nper_class_accuracy:\\n')\n",
    "        for i, c in enumerate(classes):\n",
    "            f.write(f\"{c:20s} -> {per_class_acc[i]:.6f} (n={int(cm.sum(axis=1)[i])})\\n\")\n",
    "    try:\n",
    "        import seaborn as sns\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
    "        plt.xlabel('predicted')\n",
    "        plt.ylabel('true')\n",
    "        plt.title(f'confusion matrix (acc={overall_acc:.3f})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/confusion_matrix_resnet.png', dpi=200)\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.imshow(cm, interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.xticks(np.arange(len(classes)), classes, rotation=45, ha='right')\n",
    "        plt.yticks(np.arange(len(classes)), classes)\n",
    "        plt.xlabel('predicted')\n",
    "        plt.ylabel('true')\n",
    "        plt.title(f'confusion matrix (acc={overall_acc:.3f})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/confusion_matrix_resnet.png', dpi=200)\n",
    "        plt.show()\n",
    "    print('saved results/confusion_matrix_resnet.png and results/metrics_resnet.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# 8) quick inference helper\n",
    "# -----------------\n",
    "from PIL import Image\n",
    "def predict_image(image_path, model, classes, transform, device):\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    x = transform(img).unsqueeze(0).to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        _, pred = torch.max(out, 1)\n",
    "    return classes[pred.item()]\n",
    "\n",
    "# example:\n",
    "# print(predict_image('path/to/image.jpg', model, classes, val_transform, device))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
